# LLM Universal Wrapper

A minimalistic universal wrapper for different LLM API providers.

## Purpose

This library provides a unified interface to interact with multiple LLM providers including:

**International Providers:**

- OpenAI
- Anthropic (Claude)
- Google (Gemini)
- GitHub Models (Free, no API key required)
- Hugging Face (Free tier available)
- Together AI (Free credits to start)
- Grok (X.AI)
- Groq
- OpenRouter
- Ollama (Local, no API key required)

**Chinese Providers:**

- DeepSeek (Free tier available)
- Qwen (Alibaba Cloud)
- SiliconFlow (Free tier available)

## Features

- Universal schema validation using Zod
- Provider-specific header configuration
- Consistent request/response handling
- Type-safe API interactions
- Batch processing support for cost optimization

## ğŸ”„ Batch Processing

Several providers offer batch processing capabilities that can significantly reduce costs and improve efficiency for bulk operations:

### ğŸ¢ Supported Providers

**ğŸŸ¢ OpenAI Batch API** *(âœ… Recommended)*

- **ğŸ’° Cost Savings**: 50% discount on eligible endpoints
- **â±ï¸ Processing Window**: 24 hours
- **ğŸ¯ Support**: `/v1/chat/completions`, `/v1/embeddings`, `/v1/completions`
- **ğŸ“„ Format**: JSONL file upload with custom IDs for tracking
- **ğŸ“Š Status**: Generally available with comprehensive examples

**ğŸ”µ Anthropic Message Batches** *(âœ… Recommended)*

- **ğŸ’° Cost Savings**: 50% discount on Message Batches API
- **â±ï¸ Processing Window**: Typically under 1 hour
- **ğŸ¯ Support**: All Claude models via `/v1/messages/batches`
- **ğŸ“„ Format**: JSON array of requests with custom IDs
- **ğŸ“Š Status**: Available in public beta

**ğŸŸ¡ Groq Batch Processing** *(âš ï¸ Limited)*

- **ğŸ’° Cost Savings**: 25% discount on batch requests  
- **â±ï¸ Processing Window**: 24 hours to 7 days
- **ğŸ¯ Support**: Limited model availability
- **ğŸ“Š Status**: Check latest documentation for availability

### ğŸ¯ Batch Processing Benefits

â€¢ **ğŸ’° Cost Optimization**: 25-50% savings on processing costs  
â€¢ **âš¡ Higher Throughput**: Process thousands of requests efficiently  
â€¢ **ğŸš€ Rate Limit Bypass**: Avoid individual request rate limits  
â€¢ **â° Asynchronous Processing**: Submit jobs and retrieve results later  
â€¢ **ğŸ“Š Better Resource Planning**: Predictable processing windows

### ğŸ› ï¸ Implementation Strategy

The universal wrapper will support batch processing through:

â€¢ **ğŸ“ Batch Schema Extension**: Additional batch-specific parameters  
â€¢ **ğŸ” Provider Detection**: Automatic batch capability detection  
â€¢ **ğŸ”„ Format Conversion**: Universal-to-provider batch format translation  
â€¢ **ğŸ“ˆ Status Monitoring**: Unified batch job status tracking  
â€¢ **âš™ï¸ Result Processing**: Automatic result file handling and parsing

### ğŸ’» Usage Example (Planned)

```javascript
// Batch processing (when implemented)
const batchRequest = {
  provider: 'openai',
  apiKey: 'your-key',
  batch: {
    enabled: true,
    completionWindow: '24h',
    requests: [
      {
        customId: 'req-1',
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: 'Hello' }]
      },
      // ... more requests
    ]
  }
};
```

## ğŸ’° Comprehensive Cost Analysis

### ğŸ†“ No API Key Required (Completely Free)

**ğŸ  Ollama (Local Deployment)**
â€¢ **Cost**: Completely free - no API charges, no usage limits
â€¢ **Requirements**: Local hardware (CPU/GPU) and storage space
â€¢ **Models**: Access to Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1, and hundreds more
â€¢ **Performance**: Unlimited requests limited only by your hardware capabilities
â€¢ **Best For**: Development, testing, privacy-sensitive applications, cost-conscious deployments

**ğŸ™ GitHub Models**
â€¢ **Cost**: Free access with no API key required  
â€¢ **Rate Limits**: Generous limits for individual developers
â€¢ **Models**: Access to state-of-the-art models from major providers
â€¢ **Best For**: Experimentation, learning, and prototype development

### ğŸ Free Tiers and Credits Available

**ğŸ¤— Hugging Face**
â€¢ **Free Tier**: Available with basic rate limits
â€¢ **API Key**: Optional - provides higher rate limits and priority access
â€¢ **Inference**: Free community inference endpoints available
â€¢ **Cost Optimization**: 70% savings when deployed on AWS SageMaker Spot Instances
â€¢ **Best For**: Research, model testing, community-driven projects

**ğŸ”® Together AI**
â€¢ **Free Credits**: Generous starting credits for new users
â€¢ **Pay-as-you-go**: Competitive pricing after free credits
â€¢ **Models**: Wide selection of open-source and fine-tuned models
â€¢ **Best For**: Startups and developers getting started with AI

**ğŸ§  DeepSeek**
â€¢ **Free Tier**: Available with competitive rate limits
â€¢ **Pricing**: Very competitive rates for paid usage
â€¢ **Performance**: High-quality reasoning and coding capabilities
â€¢ **Best For**: Cost-effective AI applications, especially coding and reasoning tasks

**ğŸ’« SiliconFlow**
â€¢ **Free Tier**: Available for individual developers
â€¢ **Chinese Provider**: Optimized for Asian markets
â€¢ **Models**: Access to popular open-source and proprietary models
â€¢ **Best For**: Applications targeting Chinese markets or developers in Asia

**â˜ï¸ Azure OpenAI**
â€¢ **Free Services**: Azure Cosmos DB free SKU eliminates database costs
â€¢ **Cost Control**: Detailed environment variable configurations for precise cost management
â€¢ **Enterprise Features**: Advanced security, compliance, and integration capabilities
â€¢ **Best For**: Enterprise applications requiring compliance and integration with Microsoft ecosystem

### ğŸ­ Batch Processing Cost Savings

**ğŸŸ¢ OpenAI Batch API** *(Recommended)*
â€¢ **Cost Savings**: 50% discount on eligible endpoints
â€¢ **Processing Time**: 24-hour completion window
â€¢ **Supported Endpoints**: `/v1/chat/completions`, `/v1/embeddings`, `/v1/completions`
â€¢ **Format**: JSONL file upload with custom request IDs for tracking
â€¢ **Status**: Generally available with comprehensive documentation
â€¢ **Best For**: Large-scale content processing, embeddings generation, bulk translations

**ğŸ”µ Anthropic Message Batches** *(Recommended)*
â€¢ **Cost Savings**: 50% discount on Message Batches API
â€¢ **Processing Time**: Typically under 1 hour (much faster than OpenAI)
â€¢ **Support**: All Claude models via `/v1/messages/batches` endpoint
â€¢ **Format**: JSON array of requests with custom IDs
â€¢ **Status**: Available in public beta with stable performance
â€¢ **Best For**: Bulk analysis, content moderation, research applications

**ğŸŸ¡ Groq Batch Processing** *(Limited Availability)*
â€¢ **Cost Savings**: 25% discount on batch requests
â€¢ **Processing Time**: 24 hours to 7 days (variable based on queue)
â€¢ **Limitations**: Limited model availability and capacity
â€¢ **Status**: Check latest documentation for current availability
â€¢ **Best For**: Non-urgent bulk processing when extreme speed is not required

### ğŸš€ Additional Cost Optimization Strategies

**ğŸ“Š Rate Limit Management**
â€¢ **Smart Queuing**: Implement request queuing to stay within free tier limits
â€¢ **Request Batching**: Combine multiple requests to reduce API call overhead
â€¢ **Caching**: Cache responses for repeated queries to minimize API usage
â€¢ **Load Balancing**: Distribute requests across multiple providers based on cost and availability

**âš¡ Performance Optimization**
â€¢ **Model Selection**: Choose the most cost-effective model for your specific use case
â€¢ **Token Optimization**: Minimize input/output tokens through efficient prompting
â€¢ **Streaming**: Use streaming responses to reduce perceived latency and improve UX
â€¢ **Regional Deployment**: Choose regions with lower costs or better performance

**ğŸ”„ Hybrid Approaches**
â€¢ **Development vs Production**: Use free tiers for development, paid services for production
â€¢ **Fallback Strategies**: Implement provider fallbacks based on cost and availability
â€¢ **Local + Cloud**: Use Ollama for development and testing, cloud providers for production scale

### ğŸ¯ Provider-Specific Batch Support

**âœ… Full Batch Support Available**
â€¢ **OpenAI**: Complete batch API with 50% cost savings, 24h processing, JSONL format
â€¢ **Anthropic**: Message Batches API with 50% cost savings, <1h processing, JSON format

**âš ï¸ Limited Batch Support**  
â€¢ **Groq**: 25% cost savings available but limited model selection and longer processing times

**ğŸ“¤ Individual Requests Only**
â€¢ **Google (Gemini)**: No batch processing currently available
â€¢ **Grok (X.AI)**: Individual requests only
â€¢ **OpenRouter**: Individual requests only  
â€¢ **Perplexity**: Individual requests only (but 2000 RPM standard rate limits)
â€¢ **Hugging Face**: Individual requests only (community inference)
â€¢ **Together AI**: Individual requests only
â€¢ **Qwen (Alibaba)**: Individual requests only
â€¢ **Ollama**: Local processing only (unlimited by design)
â€¢ **GitHub Models**: Individual requests only

### âš ï¸ Cost Transparency Considerations

**ğŸ” Clear Pricing Providers**
â€¢ **OpenAI**: Transparent per-token pricing with batch discounts clearly documented
â€¢ **Anthropic**: Clear pricing structure with batch savings well-documented
â€¢ **Azure OpenAI**: Enterprise pricing with detailed cost control mechanisms

**âšª Limited Pricing Transparency**
â€¢ **Grok (X.AI)**: Pricing information requires X Premium subscription context
â€¢ **GitHub Models**: Free tier limits not clearly specified in public documentation
â€¢ **OpenRouter**: Aggregates multiple providers with varying pricing structures

**ğŸ’¡ Cost Planning Recommendations**
â€¢ **Start Free**: Begin with Ollama (local) or GitHub Models for development
â€¢ **Scale Gradually**: Move to free tiers of major providers as needs grow
â€¢ **Batch When Possible**: Use batch processing for any bulk operations (50% savings)
â€¢ **Monitor Usage**: Implement usage tracking and alerts for cost control
â€¢ **Plan for Growth**: Design applications to easily switch between providers based on cost and performance needs

## Usage

Define your input schema, provide your data, and let the wrapper handle provider-specific formatting and requests.

Copy your API keys to `.env` file:

```env
# Required API keys
OPENAI_API_KEY=
GEMINI_API_KEY=
GROK_API_KEY=
GROQ_API_KEY=
OPENROUTER_API_KEY=
ANTHROPIC_API_KEY=
TOGETHER_API_KEY=

# Chinese providers
DEEPSEEK_API_KEY=
# Alibaba Cloud DashScope API key
QWEN_API_KEY=
SILICONFLOW_API_KEY=

# Optional API keys (providers offer free tiers)
# Optional - for higher rate limits on GitHub Models
GITHUB_TOKEN=
# Optional - for higher rate limits on Hugging Face
HUGGINGFACE_API_KEY=

# Ollama configuration (local deployment)
# Note: OLLAMA_API_KEY is optional for most local Ollama setups
OLLAMA_API_KEY=
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```
