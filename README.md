# LLM Universal Wrapper

A minimalistic universal wrapper for different LLM API providers.

## Purpose

This library provides a unified interface to interact with multiple LLM providers including:

**International Providers:**

- OpenAI
- Anthropic (Claude)
- Google (Gemini)
- GitHub Models (Free, no API key required)
- Hugging Face (Free tier available)
- Together AI (Free credits to start)
- Grok (X.AI)
- Groq
- OpenRouter
- Ollama (Local, no API key required)

**Chinese Providers:**

- DeepSeek (Free tier available)
- Qwen (Alibaba Cloud)
- SiliconFlow (Free tier available)

## Features

- Universal schema validation using Zod
- Provider-specific header configuration
- Consistent request/response handling
- Type-safe API interactions
- Batch processing support for cost optimization

## ğŸ”„ Batch Processing

Several providers offer batch processing capabilities that can significantly reduce costs and improve efficiency for bulk operations:

### ğŸ¢ Supported Providers

**ğŸŸ¢ OpenAI Batch API** *(âœ… Recommended)*

- **ğŸ’° Cost Savings**: 50% discount on eligible endpoints
- **â±ï¸ Processing Window**: 24 hours
- **ğŸ¯ Support**: `/v1/chat/completions`, `/v1/embeddings`, `/v1/completions`
- **ğŸ“„ Format**: JSONL file upload with custom IDs for tracking
- **ğŸ“Š Status**: Generally available with comprehensive examples

**ğŸ”µ Anthropic Message Batches** *(âœ… Recommended)*

- **ğŸ’° Cost Savings**: 50% discount on Message Batches API
- **â±ï¸ Processing Window**: Typically under 1 hour
- **ğŸ¯ Support**: All Claude models via `/v1/messages/batches`
- **ğŸ“„ Format**: JSON array of requests with custom IDs
- **ğŸ“Š Status**: Available in public beta

**ğŸŸ¡ Groq Batch Processing** *(âš ï¸ Limited)*

- **ğŸ’° Cost Savings**: 25% discount on batch requests  
- **â±ï¸ Processing Window**: 24 hours to 7 days
- **ğŸ¯ Support**: Limited model availability
- **ğŸ“Š Status**: Check latest documentation for availability

### ğŸ¯ Batch Processing Benefits

â€¢ **ğŸ’° Cost Optimization**: 25-50% savings on processing costs  
â€¢ **âš¡ Higher Throughput**: Process thousands of requests efficiently  
â€¢ **ğŸš€ Rate Limit Bypass**: Avoid individual request rate limits  
â€¢ **â° Asynchronous Processing**: Submit jobs and retrieve results later  
â€¢ **ğŸ“Š Better Resource Planning**: Predictable processing windows

### ğŸ› ï¸ Implementation Strategy

The universal wrapper will support batch processing through:

â€¢ **ğŸ“ Batch Schema Extension**: Additional batch-specific parameters  
â€¢ **ğŸ” Provider Detection**: Automatic batch capability detection  
â€¢ **ğŸ”„ Format Conversion**: Universal-to-provider batch format translation  
â€¢ **ğŸ“ˆ Status Monitoring**: Unified batch job status tracking  
â€¢ **âš™ï¸ Result Processing**: Automatic result file handling and parsing

### ğŸ’» Usage Example (Planned)

```javascript
// Batch processing (when implemented)
const batchRequest = {
  provider: 'openai',
  apiKey: 'your-key',
  batch: {
    enabled: true,
    completionWindow: '24h',
    requests: [
      {
        customId: 'req-1',
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: 'Hello' }]
      },
      // ... more requests
    ]
  }
};
```

### ğŸ“Š Provider Support Matrix

| Provider    | Batch Support  | Cost Savings | Processing Time | Status              |
|-------------|----------------|--------------|-----------------|---------------------|
| OpenAI      | âœ… **Full**    | **50%**      | 24h             | âœ… Available        |
| Anthropic   | âœ… **Full**    | **50%**      | <1h             | ğŸ§ª Beta             |
| Groq        | âš ï¸ **Limited** | **25%**      | 24h-7d          | âš ï¸ Limited          |
| Others      | âŒ None        | -            | -               | ğŸ“¤ Individual only  |

## Usage

Define your input schema, provide your data, and let the wrapper handle provider-specific formatting and requests.

Copy your API keys to `.env` file:

```env
# Required API keys
OPENAI_API_KEY=your_key_here
GEMINI_API_KEY=your_key_here
GROK_API_KEY=your_key_here
GROQ_API_KEY=your_key_here
OPENROUTER_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
TOGETHER_API_KEY=your_key_here

# Chinese providers
DEEPSEEK_API_KEY=your_key_here
QWEN_API_KEY=your_key_here  # Alibaba Cloud DashScope API key
SILICONFLOW_API_KEY=your_key_here

# Optional API keys (providers offer free tiers)
GITHUB_TOKEN=your_token_here  # Optional - for higher rate limits on GitHub Models
HUGGINGFACE_API_KEY=your_key_here  # Optional - for higher rate limits on Hugging Face

# Ollama configuration (local deployment)
# Note: OLLAMA_API_KEY is optional for most local Ollama setups
OLLAMA_API_KEY=your_key_here  
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```
